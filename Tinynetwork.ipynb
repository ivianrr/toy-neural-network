{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.samsonzhang.com/2020/11/24/understanding-the-math-behind-neural-networks-by-building-one-from-scratch-no-tf-keras-just-numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data():\n",
    "    with open('samples/t10k-images.idx3-ubyte','rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        data = data.reshape((size, -1))\n",
    "        data=data/255\n",
    "    with open('samples/t10k-labels.idx1-ubyte','rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        labels = labels.reshape((size,)) # (Optional)\n",
    "    return labels, data, nrows, ncols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most contexts, including for machine learning, the convention is to stack these vectors as rows of the matrix, giving the matrix dimensions of $m : \\text{rows} \\times n : \\text{columns}$, where $m$ is the number of training examples and $n$ is the number of features, in this case 784. To make our math easier, we're going to transpose this matrix, giving it dimensions $n \\times m$ instead, with each column corresponding to a training example and each row a training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000, 784)\n",
      "(20,)\n",
      "(784, 20)\n"
     ]
    }
   ],
   "source": [
    "labels, data, nrows, ncols=load_data()\n",
    "\n",
    "# m:rows:examples, n:columns:features\n",
    "print(labels.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# We transpose it to make math easier\n",
    "data=data.T[:,:20]\n",
    "labels=labels[:20]\n",
    "print(labels.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview of first data point\n",
    "plt.imshow(data[:,0].reshape(nrows,ncols), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing weights and biases\n",
    "\n",
    "Let's look at our neural network now. Between every two layers is a set of connections between every node in the previous layer and every node in the following one. That is, there is a weight $w_{i,j}$ for every $i$ in the number of nodes in the previous layer and every $j$ in the number of nodes in the following one.\n",
    "\n",
    "It's natural, then, to represent our weights as a matrix of dimensions $n^{[l]}\\times n^{[l-1]}$, where $n^{[l-1]}$ is the number of nodes in the previous layer and $n^{[l]}$ is the number of nodes in the following layer. Let's call this matrix $W^{[l]}$, corresponding to layer $l$ of our network. $W^{[1]}$, for example, will be a $10\\times784$ matrix, taking us from the 784 nodes of the input layer to the 10 nodes of the first hidden layer. $W^{[2]}$ will have dimensions $10\\times10$.\n",
    "\n",
    "Biases are simply constant terms added to each node of the following layer, so we can represent it as a matrix with dimensions $n^{[l]}\\times 1$. Let's call these matrices $b^{[l]}$, so that $b^{[1]}$ and $b^{[2]}$ both have dimensions $10\\times1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "W1, b1, W2, b2 =init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "With these representations in mind, we can now write the equations for forward propagation.\n",
    "\n",
    "First, we'll compute the unactivated values of the nodes in the first hidden layer by applying $W^{[1]}$ and $b^{[1]}$ to our input layer. We'll call the output of this operation $Z^{[1]}$:\n",
    "\n",
    "$Z^{[1]} = W^{[1]}X+b^{[1]}$\n",
    "\n",
    "Remember that $X$ has dimensions $784\\times m$, and $W^{[1]} : 10\\times784$. $W^{[1]}X$ is the dot product between the two, yielding a new matrix of dimensions $10\\times m$. This may seem a little strange at first, but think of it this way: each column of this matrix corresponds to the unactivated values for the nodes in the first hidden layer when carried out for one training example, so the entire matrix represents carrying out the first step of forward propagation for all training examples at the same time. This is much more efficient than a for loop, and is what was referred to earlier as a \"vectorized implementation.\"\n",
    "\n",
    "Our bias term $b^{[1]}$has dimensions $10\\times1$, but we want the same column of biases to be applied to all $m$ columns of training examples, so $b^{[1]}$is effectively broadcast into a matrix of dimensions $10\\times m$ when calculating $Z^{[1]}$, matching the dimensions of $W^{[1]}X$.\n",
    "\n",
    "We need to do one more calculation before moving on to the next layer, though, and that's applying a non-linear activation to $Z^{[1]}$. What does this mean, and why do we have to do it?\n",
    "\n",
    "Imagine that we didn't do anything to $Z^{[1]}$ now, and multiplied it by $W^{[2]}$ and added $b^{[2]}$ to get the value for the next layer. $Z^{[1]}$ is a linear combination of the input features, and the second layer would be a linear combination of $Z^{[1]}$, making it still a linear combination of the input features. That means that our hidden layer is essentially useless, and we're just building a linear regression model.\n",
    "\n",
    "To prevent this reduction and actually add complexity with our layers, we'll run $Z^{[1]}$ through a non-linear activation function before passing it off to the next layer. In this case, we'll be using a function called a rectified linear unit, or ReLU:\n",
    "\n",
    "\n",
    "\n",
    "ReLU is a really simple function: it's linear if the input value is above 0, and outputs 0 otherwise. Just this much, though, is enough to ensure that our model doesn't collapse to a linear one.\n",
    "\n",
    "From $Z^{[1]}$, we'll calculate a value $A^{[1]}$ for the values of the nodes in the hidden layer of our neural network after applying our activation function to it:\n",
    "\n",
    "$A^{[1]} = \\text{ReLU}(Z^{[1]})$\n",
    "\n",
    "More generally, you might see this written as $A^{[l]} = g(Z^{[l]})$, with $g$ referring to an arbitrary activation function that may be something other than ReLU.\n",
    "\n",
    "Once we have $A^{[1]}$, we can proceed to calculating the values for our second layer, which is also our output layer. First, we calculate $Z^{[2]}$:\n",
    "\n",
    "$Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]}$\n",
    "\n",
    "Then, we'll apply an activation function to $Z^{[2]}$ to get our final output.\n",
    "\n",
    "If this second layer were just another hidden layer, with more hidden layers or an output layer after it, we would apply ReLU again. But since it's the output layer, we'll apply a special activation function called softmax:\n",
    "\n",
    "\n",
    "\n",
    "Diagram by Bartosz Szabłowski on Towards Data Science\n",
    "\n",
    "Softmax takes a column of data at a time, taking each element in the column and outputting the exponential of that element divided by the sum of the exponentials of each of the elements in the input column. The end result is a column of probabilities between 0 and 1.\n",
    "\n",
    "The value of using softmax for our output layer is that we can read the output as probabilities for certain predictions. In the diagram above, for example, we might read the output as a prediction that the second class has a 90% probability of being the correct label, the third a 5% probability, the fourth a 1% probability, and so on.\n",
    "\n",
    "Let's find our $A^{[2]}$:\n",
    "\n",
    "$A^{[2]} = \\text{softmax}(Z^{[2]})$\n",
    "\n",
    "Softmax runs element-wise, so the dimensions of $Z^{[2]}$ are preserved at $10\\times m$. We can read this output matrix as follows: value $A^{[2]}_{i, j}$ is the probability that example $j$ is an image of the digit $i$.\n",
    "\n",
    "With that, we've run through the entire neural network, going from our input $X$ containing all of our training examples to an output matrix $A^{[2]}$ containing prediction probabilities for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "def forward_prop(W1,b1,W2,b2,X):\n",
    "    Z1=W1.dot(X)+b1\n",
    "    A1=ReLU(Z1)\n",
    "    Z2=W2.dot(A1)+b2\n",
    "    A2=softmax(Z2)\n",
    "    return Z1,A1,Z2,A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1=W1.dot(data)+b1\n",
    "Z1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1,A1,Z2,A2=forward_prop(W1,b1,W2,b2,data)\n",
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(10, 20)\n",
      "7 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y=np.array([2,3,0,5,9])\n",
    "print(y.shape)\n",
    "def one_hot(y):\n",
    "    yy=np.zeros((y.max()+1,y.size))\n",
    "    yy[y,np.arange(y.size)]=1\n",
    "    return yy\n",
    "one_hot_y=one_hot(labels)\n",
    "print(one_hot_y.shape)\n",
    "print(labels[0],one_hot_y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "[3.52404311 3.28049731 3.36230285 3.40057685 1.21818377 4.62607283\n",
      " 3.63665799 6.05865523 1.5267316  4.88458003 0.93981687 2.66053854\n",
      " 6.32452152 2.95812434 5.77313217 1.66769807 3.90546879 6.03564035\n",
      " 1.9933341  1.10749383]\n",
      "(20,)\n",
      "[3.52404311 3.28049731 3.36230285 3.40057685 1.21818377 4.62607283\n",
      " 3.63665799 6.05865523 1.5267316  4.88458003 0.93981687 2.66053854\n",
      " 6.32452152 2.95812434 5.77313217 1.66769807 3.90546879 6.03564035\n",
      " 1.9933341  1.10749383]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_ent(y,y_hat): # true(onehotencoded), predicted\n",
    "    return np.sum(-np.log(y_hat)*y,axis=0)\n",
    "\n",
    "ce=cross_ent(one_hot_y,A2)\n",
    "print(ce.shape)\n",
    "print(ce)\n",
    "\n",
    "def cross_ent2(y,y_hat): # true(onehotencoded), predicted\n",
    "    return -np.log(y_hat.T[y.astype(bool).T])\n",
    "ce=cross_ent2(one_hot_y,A2)\n",
    "print(ce.shape)\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.40057685, 0.93981687, 2.95812434, 3.36230285, 4.62607283,\n",
       "       5.77313217, 3.28049731, 1.9933341 , 1.21818377, 3.63665799,\n",
       "       1.10749383, 1.5267316 , 1.66769807, 2.66053854, 3.52404311,\n",
       "       6.03564035, 6.05865523, 4.88458003, 6.32452152, 3.90546879])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(A2[one_hot_y.astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03335402, 0.39069938, 0.0519162 , 0.03465536, 0.00979314,\n",
       "       0.00311   , 0.03760955, 0.13624043, 0.29576686, 0.02634023,\n",
       "       0.33038593, 0.21724455, 0.1886809 , 0.06991056, 0.02948   ,\n",
       "       0.00239196, 0.00233754, 0.0075623 , 0.00179182, 0.02013151])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2[one_hot_y.astype(bool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21817525, 0.00610508, 0.18961005, 0.03788545, 0.09771088,\n",
       "       0.22865676, 0.13942102, 0.02948   , 0.01360296, 0.03935254])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y.astype(bool).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[False False False]\n",
      " [False  True False]\n",
      " [False False False]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(9).reshape(3,3)\n",
    "a\n",
    "a_m=np.zeros_like(a).astype(bool)\n",
    "a_m[1,1]=True\n",
    "print(a)\n",
    "print(a_m)\n",
    "a[a_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "Now, we'll go the opposite way and calculate how to nudge our parameters to carry out gradient descent.\n",
    "\n",
    "Mathematically, what we're actually computing is the derivative of the loss function with respect to each weight and bias parameter. For a softmax classifier, we'll use a cross-entropy loss function:\n",
    "\n",
    "$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector. It might look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0.01 \\ 0.02 \\ 0.05 \\ 0.02 \\ 0.80 \\ 0.01 \\ 0.01 \\ 0.00 \\ 0.01 \\ 0.07 \\ \\end{bmatrix}$\n",
    "\n",
    "$y$ is the one-hot encoding of the correct label for the training example. If the label for a training example is 4, for example, the one-hot encoding of $y$ would look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}$\n",
    "\n",
    "Notice that in our sum $\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$, $y_i = 0$ for all $i$ except the correct label. The loss for a given example, then, is just the log of the probability given for the correct prediction. In our example above, $J(\\hat{y}, y) = -\\log(y_4) = -\\log(0.80) \\approx 0.097$. Notice that, the closer the prediction probability is to 1, the closer the loss is to 0. As the probability approaches 0, the loss approaches $+\\infty$. By minimizing the cost function, we improve the accuracy of our model. We do so by substracting the derivative of the loss function with respect to each parameter from that parameter over many rounds of graident descent:\n",
    "\n",
    "$W^{[1]} := W^{[1]} - \\alpha \\frac{\\delta J}{\\delta W^{[1]}} \\ b^{[1]} := b^{[1]} - \\alpha \\frac{\\delta J}{\\delta b^{[1]}} \\ W^{[2]} := W^{[2]} - \\alpha \\frac{\\delta J}{\\delta W^{[2]}} \\ b^{[2]} := b^{[2]} - \\alpha \\frac{\\delta J}{\\delta b^{[2]}} \\ $\n",
    "\n",
    "Our objective in backprop is to find $\\frac{\\delta J}{\\delta W^{[1]}},\\frac{\\delta J}{\\delta b^{[1]}},\\frac{\\delta J}{\\delta W^{[2]}},$ and $\\frac{\\delta J}{\\delta b^{[2]}}$. For concision, we'll write these values as $dW^{[1]}, db^{[1]}, dW^{[2]},$ and $db^{[2]}$. We'll find these values by stepping backwards through our network, starting by calculating $\\frac{\\delta J}{\\delta A^{[2]}}$, or $dA^{[2]}$. Turns out that this derivative is simply:\n",
    "\n",
    "$dA^{[2]} = Y - A^{[2]}$\n",
    "\n",
    "If you know calculus, you can take the derivative of the loss function and confirm this for yourself. (Hint: $\\hat{y} = A^{[2]}$)\n",
    "\n",
    "From $dA^{[2]}$, we can calculate $dW^{[2]}$ and $db^{[1]}$:\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T} \\ dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$\n",
    "\n",
    "Then, to calculate $dW^{[1]}$ and $db^{[1]}$, we'll first find $dZ^{[1]}$:\n",
    "\n",
    "$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (Z^{[1]})$\n",
    "\n",
    "I won't explain all the details of the math, but you can get some intuitive hints at what's going on here by just looking at the variables. We're applying $W^{[2]T}$ to $dZ^{[2]}$, akin to applying the weights between layers 1 and 2 in reverse. Then, we perform an element-wise multiplication with the derivative of the activation function, akin to \"undoing\" it to get the correct error values.\n",
    "\n",
    "Since our activation function is ReLU, our derivative is actually pretty simple. Let's revisit our graph:\n",
    "\n",
    "\n",
    "\n",
    "When the input value is greater than 0, the activation function is linear with a derivative of 1. When the input value is less than 0, the activation function is horizontal with a derivative of 0. Thus, $g^{[1]\\prime}(Z^{[1]})$ is just a matrix of 1s and 0s based on values of $Z^{[1]}$.\n",
    "\n",
    "From here, we do the same calculations as earlier to find $dW^{[1]}$ and $db^{[1]}$, using $X$ in place of $A^{[1]}$:\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T \\ dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Now we've found all the derivatives we need, and all that's left is to update our parameters:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "Here, $\\alpha$is our learning rate, a \"hyperparameter\" that we set to whatever we want. $\\alpha$ is distinguished from other parameters because, just like the number of layers in the network or the number of units in each layer, it's a value that we choose for our model rather than one that gradient descent optimizes.\n",
    "\n",
    "With that, we've gone over all the math that we need to carry out gradient descent and train our neural network. To recap: first, we carry out forward propagation, getting a prediction from an input image:\n",
    "\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]}A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))\\Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$\n",
    "\n",
    "Then, we carry out backprop to compute loss function derivatives:\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Finally, we update our parameters accordingly:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "We'll do this process over and over again — the exact number of times, an iteration count that we again set ourselves — until we are satisfied with the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('calculoastronomico')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45191bebbaaca2a45e1ef8d4f69ed6ce1b861e31669d6b1375d42c01e883be29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
