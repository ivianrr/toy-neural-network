{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.samsonzhang.com/2020/11/24/understanding-the-math-behind-neural-networks-by-building-one-from-scratch-no-tf-keras-just-numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data():\n",
    "    with open('samples/t10k-images.idx3-ubyte','rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        nrows, ncols = struct.unpack(\">II\", f.read(8))\n",
    "        data = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        data = data.reshape((size, -1))\n",
    "        data=data/255\n",
    "    with open('samples/t10k-labels.idx1-ubyte','rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.dtype(np.uint8).newbyteorder('>'))\n",
    "        labels = labels.reshape((size,)) # (Optional)\n",
    "    return labels, data, nrows, ncols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most contexts, including for machine learning, the convention is to stack these vectors as rows of the matrix, giving the matrix dimensions of $m : \\text{rows} \\times n : \\text{columns}$, where $m$ is the number of training examples and $n$ is the number of features, in this case 784. To make our math easier, we're going to transpose this matrix, giving it dimensions $n \\times m$ instead, with each column corresponding to a training example and each row a training feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000, 784)\n",
      "(10000,)\n",
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "labels, data, nrows, ncols=load_data()\n",
    "n_samples=10000\n",
    "# m:rows:examples, n:columns:features\n",
    "print(labels.shape)\n",
    "print(data.shape)\n",
    "\n",
    "# We transpose it to make math easier\n",
    "data=data.T[:,:n_samples]\n",
    "labels=labels[:n_samples]\n",
    "print(labels.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview of first data point\n",
    "plt.imshow(data[:,0].reshape(nrows,ncols), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing weights and biases\n",
    "\n",
    "Let's look at our neural network now. Between every two layers is a set of connections between every node in the previous layer and every node in the following one. That is, there is a weight $w_{i,j}$ for every $i$ in the number of nodes in the previous layer and every $j$ in the number of nodes in the following one.\n",
    "\n",
    "It's natural, then, to represent our weights as a matrix of dimensions $n^{[l]}\\times n^{[l-1]}$, where $n^{[l-1]}$ is the number of nodes in the previous layer and $n^{[l]}$ is the number of nodes in the following layer. Let's call this matrix $W^{[l]}$, corresponding to layer $l$ of our network. $W^{[1]}$, for example, will be a $10\\times784$ matrix, taking us from the 784 nodes of the input layer to the 10 nodes of the first hidden layer. $W^{[2]}$ will have dimensions $10\\times10$.\n",
    "\n",
    "Biases are simply constant terms added to each node of the following layer, so we can represent it as a matrix with dimensions $n^{[l]}\\times 1$. Let's call these matrices $b^{[l]}$, so that $b^{[1]}$ and $b^{[2]}$ both have dimensions $10\\times1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    N_L1=10\n",
    "    N_L2=10\n",
    "    W1 = np.random.rand(N_L1, 784) - 0.5\n",
    "    b1 = np.random.rand(N_L1, 1) - 0.5\n",
    "    W2 = np.random.rand(N_L2, N_L1) - 0.5\n",
    "    b2 = np.random.rand(N_L2, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "W1, b1, W2, b2 =init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "With these representations in mind, we can now write the equations for forward propagation.\n",
    "\n",
    "First, we'll compute the unactivated values of the nodes in the first hidden layer by applying $W^{[1]}$ and $b^{[1]}$ to our input layer. We'll call the output of this operation $Z^{[1]}$:\n",
    "\n",
    "$Z^{[1]} = W^{[1]}X+b^{[1]}$\n",
    "\n",
    "Remember that $X$ has dimensions $784\\times m$, and $W^{[1]} : 10\\times784$. $W^{[1]}X$ is the dot product between the two, yielding a new matrix of dimensions $10\\times m$. This may seem a little strange at first, but think of it this way: each column of this matrix corresponds to the unactivated values for the nodes in the first hidden layer when carried out for one training example, so the entire matrix represents carrying out the first step of forward propagation for all training examples at the same time. This is much more efficient than a for loop, and is what was referred to earlier as a \"vectorized implementation.\"\n",
    "\n",
    "Our bias term $b^{[1]}$has dimensions $10\\times1$, but we want the same column of biases to be applied to all $m$ columns of training examples, so $b^{[1]}$is effectively broadcast into a matrix of dimensions $10\\times m$ when calculating $Z^{[1]}$, matching the dimensions of $W^{[1]}X$.\n",
    "\n",
    "We need to do one more calculation before moving on to the next layer, though, and that's applying a non-linear activation to $Z^{[1]}$. What does this mean, and why do we have to do it?\n",
    "\n",
    "Imagine that we didn't do anything to $Z^{[1]}$ now, and multiplied it by $W^{[2]}$ and added $b^{[2]}$ to get the value for the next layer. $Z^{[1]}$ is a linear combination of the input features, and the second layer would be a linear combination of $Z^{[1]}$, making it still a linear combination of the input features. That means that our hidden layer is essentially useless, and we're just building a linear regression model.\n",
    "\n",
    "To prevent this reduction and actually add complexity with our layers, we'll run $Z^{[1]}$ through a non-linear activation function before passing it off to the next layer. In this case, we'll be using a function called a rectified linear unit, or ReLU:\n",
    "\n",
    "\n",
    "\n",
    "ReLU is a really simple function: it's linear if the input value is above 0, and outputs 0 otherwise. Just this much, though, is enough to ensure that our model doesn't collapse to a linear one.\n",
    "\n",
    "From $Z^{[1]}$, we'll calculate a value $A^{[1]}$ for the values of the nodes in the hidden layer of our neural network after applying our activation function to it:\n",
    "\n",
    "$A^{[1]} = \\text{ReLU}(Z^{[1]})$\n",
    "\n",
    "More generally, you might see this written as $A^{[l]} = g(Z^{[l]})$, with $g$ referring to an arbitrary activation function that may be something other than ReLU.\n",
    "\n",
    "Once we have $A^{[1]}$, we can proceed to calculating the values for our second layer, which is also our output layer. First, we calculate $Z^{[2]}$:\n",
    "\n",
    "$Z^{[2]} = W^{[2]}A^{[1]}+b^{[2]}$\n",
    "\n",
    "Then, we'll apply an activation function to $Z^{[2]}$ to get our final output.\n",
    "\n",
    "If this second layer were just another hidden layer, with more hidden layers or an output layer after it, we would apply ReLU again. But since it's the output layer, we'll apply a special activation function called softmax:\n",
    "\n",
    "\n",
    "\n",
    "Diagram by Bartosz Szabłowski on Towards Data Science\n",
    "\n",
    "Softmax takes a column of data at a time, taking each element in the column and outputting the exponential of that element divided by the sum of the exponentials of each of the elements in the input column. The end result is a column of probabilities between 0 and 1.\n",
    "\n",
    "The value of using softmax for our output layer is that we can read the output as probabilities for certain predictions. In the diagram above, for example, we might read the output as a prediction that the second class has a 90% probability of being the correct label, the third a 5% probability, the fourth a 1% probability, and so on.\n",
    "\n",
    "Let's find our $A^{[2]}$:\n",
    "\n",
    "$A^{[2]} = \\text{softmax}(Z^{[2]})$\n",
    "\n",
    "Softmax runs element-wise, so the dimensions of $Z^{[2]}$ are preserved at $10\\times m$. We can read this output matrix as follows: value $A^{[2]}_{i, j}$ is the probability that example $j$ is an image of the digit $i$.\n",
    "\n",
    "With that, we've run through the entire neural network, going from our input $X$ containing all of our training examples to an output matrix $A^{[2]}$ containing prediction probabilities for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    return np.maximum(Z,0)\n",
    "\n",
    "def ReLU_dev(Z):\n",
    "    return 1*(Z>0)\n",
    "\n",
    "def softmax(Z):\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "def forward_prop(W1,b1,W2,b2,X):\n",
    "    Z1=W1.dot(X)+b1\n",
    "    A1=ReLU(Z1)\n",
    "    Z2=W2.dot(A1)+b2\n",
    "    A2=softmax(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "\n",
    "def get_predictions(A2):\n",
    "    return np.argmax(A2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1,A1,Z2,A2=forward_prop(W1,b1,W2,b2,data)\n",
    "A2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10000)\n",
      "7 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def one_hot(y):\n",
    "    yy=np.zeros((y.max()+1,y.size))\n",
    "    yy[y,np.arange(y.size)]=1\n",
    "    return yy\n",
    "one_hot_y=one_hot(labels)\n",
    "print(one_hot_y.shape)\n",
    "print(labels[0],one_hot_y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "[3.09331042 6.50347338 7.512184   ... 6.15864317 8.25185466 1.30515616]\n"
     ]
    }
   ],
   "source": [
    "def cross_ent(y,y_hat): # true(onehotencoded), predicted\n",
    "    return -np.log(y_hat.T[y.astype(bool).T])\n",
    "\n",
    "ce=cross_ent(one_hot_y,A2)\n",
    "\n",
    "print(ce.shape)\n",
    "print(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "Now, we'll go the opposite way and calculate how to nudge our parameters to carry out gradient descent.\n",
    "\n",
    "Mathematically, what we're actually computing is the derivative of the loss function with respect to each weight and bias parameter. For a softmax classifier, we'll use a cross-entropy loss function:\n",
    "\n",
    "$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Here, $\\hat{y}$ is our prediction vector. It might look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0.01 \\ 0.02 \\ 0.05 \\ 0.02 \\ 0.80 \\ 0.01 \\ 0.01 \\ 0.00 \\ 0.01 \\ 0.07 \\ \\end{bmatrix}$\n",
    "\n",
    "$y$ is the one-hot encoding of the correct label for the training example. If the label for a training example is 4, for example, the one-hot encoding of $y$ would look like this:\n",
    "\n",
    "$\\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}$\n",
    "\n",
    "Notice that in our sum $\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$, $y_i = 0$ for all $i$ except the correct label. The loss for a given example, then, is just the log of the probability given for the correct prediction. In our example above, $J(\\hat{y}, y) = -\\log(y_4) = -\\log(0.80) \\approx 0.097$. Notice that, the closer the prediction probability is to 1, the closer the loss is to 0. As the probability approaches 0, the loss approaches $+\\infty$. By minimizing the cost function, we improve the accuracy of our model. We do so by substracting the derivative of the loss function with respect to each parameter from that parameter over many rounds of graident descent:\n",
    "\n",
    "$W^{[1]} := W^{[1]} - \\alpha \\frac{\\delta J}{\\delta W^{[1]}} \\ b^{[1]} := b^{[1]} - \\alpha \\frac{\\delta J}{\\delta b^{[1]}} \\ W^{[2]} := W^{[2]} - \\alpha \\frac{\\delta J}{\\delta W^{[2]}} \\ b^{[2]} := b^{[2]} - \\alpha \\frac{\\delta J}{\\delta b^{[2]}} \\ $\n",
    "\n",
    "Our objective in backprop is to find $\\frac{\\delta J}{\\delta W^{[1]}},\\frac{\\delta J}{\\delta b^{[1]}},\\frac{\\delta J}{\\delta W^{[2]}},$ and $\\frac{\\delta J}{\\delta b^{[2]}}$. For concision, we'll write these values as $dW^{[1]}, db^{[1]}, dW^{[2]},$ and $db^{[2]}$. We'll find these values by stepping backwards through our network, starting by calculating $\\frac{\\delta J}{\\delta A^{[2]}}$, or $dA^{[2]}$. Turns out that this derivative is simply:\n",
    "\n",
    "$dA^{[2]} = Y - A^{[2]}$\n",
    "\n",
    "If you know calculus, you can take the derivative of the loss function and confirm this for yourself. (Hint: $\\hat{y} = A^{[2]}$)\n",
    "\n",
    "From $dA^{[2]}$, we can calculate $dW^{[2]}$ and $db^{[1]}$:\n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T} \\ dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$\n",
    "\n",
    "Then, to calculate $dW^{[1]}$ and $db^{[1]}$, we'll first find $dZ^{[1]}$:\n",
    "\n",
    "$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (Z^{[1]})$\n",
    "\n",
    "I won't explain all the details of the math, but you can get some intuitive hints at what's going on here by just looking at the variables. We're applying $W^{[2]T}$ to $dZ^{[2]}$, akin to applying the weights between layers 1 and 2 in reverse. Then, we perform an element-wise multiplication with the derivative of the activation function, akin to \"undoing\" it to get the correct error values.\n",
    "\n",
    "Since our activation function is ReLU, our derivative is actually pretty simple. Let's revisit our graph:\n",
    "\n",
    "\n",
    "\n",
    "When the input value is greater than 0, the activation function is linear with a derivative of 1. When the input value is less than 0, the activation function is horizontal with a derivative of 0. Thus, $g^{[1]\\prime}(Z^{[1]})$ is just a matrix of 1s and 0s based on values of $Z^{[1]}$.\n",
    "\n",
    "From here, we do the same calculations as earlier to find $dW^{[1]}$ and $db^{[1]}$, using $X$ in place of $A^{[1]}$:\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T \\ dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Now we've found all the derivatives we need, and all that's left is to update our parameters:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "Here, $\\alpha$is our learning rate, a \"hyperparameter\" that we set to whatever we want. $\\alpha$ is distinguished from other parameters because, just like the number of layers in the network or the number of units in each layer, it's a value that we choose for our model rather than one that gradient descent optimizes.\n",
    "\n",
    "With that, we've gone over all the math that we need to carry out gradient descent and train our neural network. To recap: first, we carry out forward propagation, getting a prediction from an input image:\n",
    "\n",
    "$Z^{[1]} = W^{[1]} X + b^{[1]}A^{[1]} = g_{\\text{ReLU}}(Z^{[1]}))\\Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}A^{[2]} = g_{\\text{softmax}}(Z^{[2]})$\n",
    "\n",
    "Then, we carry out backprop to compute loss function derivatives:\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$\n",
    "\n",
    "Finally, we update our parameters accordingly:\n",
    "\n",
    "$W^{[2]} := W^{[2]} - \\alpha dW^{[2]} \\ b^{[2]} := b^{[2]} - \\alpha db^{[2]} \\ W^{[1]} := W^{[1]} - \\alpha dW^{[1]} \\ b^{[1]} := b^{[1]} - \\alpha db^{[1]}$\n",
    "\n",
    "We'll do this process over and over again — the exact number of times, an iteration count that we again set ourselves — until we are satisfied with the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy\n",
    "![](d1.png)\n",
    "![](d2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 784)\n",
      "(20, 1)\n",
      "(10, 20)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "# forward_prop(W1,b1,W2,b2,X):\n",
    "# return Z1,A1,Z2,A2\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    N=labels.size\n",
    "    one_hot_y=one_hot(Y)\n",
    "    dZ2=A2-one_hot_y\n",
    "    db2=np.sum(dZ2,axis=1,keepdims=True)/N\n",
    "    dW2=dZ2.dot(A1.T)/N\n",
    "    dZ1=W2.T.dot(dZ2)*ReLU_dev(Z1)\n",
    "    db1=np.sum(dZ1,axis=1,keepdims=True)/N\n",
    "    dW1=dZ1.dot(X.T)/N\n",
    "    return dW1,db1,dW2,db2\n",
    "\n",
    "for i in backward_prop(Z1, A1, Z2, A2, W1, W2, data, labels):\n",
    "    print(i.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW1,db1,dW2,db2=backward_prop(Z1, A1, Z2, A2, W1, W2, data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1,b1, W2,b2,dW1,db1,dW2,db2,alpha):\n",
    "    W1-=dW1*alpha\n",
    "    b1-=db1*alpha\n",
    "    W2-=dW2*alpha\n",
    "    b2-=db2*alpha\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions,Y):\n",
    "    return np.sum(predictions==Y)/Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8, 5, 6, ..., 6, 3, 3],\n",
       "       [7, 2, 1, ..., 4, 5, 6]], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=get_predictions(A2)\n",
    "acc=get_accuracy(predictions,labels)\n",
    "print(acc)\n",
    "np.stack((predictions,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,alpha,iterations):\n",
    "    W1, b1, W2, b2 =init_params()\n",
    "    debug=[]\n",
    "    for i in range(iterations):\n",
    "        Z1,A1,Z2,A2 = forward_prop(W1,b1,W2,b2,X)\n",
    "        dW1,db1,dW2,db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1,b1, W2,b2,dW1,db1,dW2,db2,alpha)\n",
    "    \n",
    "        if i%10==0 or i==iterations-1:\n",
    "            predictions=get_predictions(A2)\n",
    "            acc=get_accuracy(predictions,labels)\n",
    "            debug.append((i,acc))\n",
    "            print(f\"Iteration: {i}\\t\\tAccuracy: {acc}\")\n",
    "\n",
    "    return W1,b1,W2,b2,debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,1) (10,1) (20,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\notebooks\\personal\\Machine learning\\Tiny network\\mininetwork.ipynb Celda 21\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m labels, data, _,_ \u001b[39m=\u001b[39mload_data()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mT\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m W1,b1,W2,b2,debug\u001b[39m=\u001b[39mgradient_descent(data,labels,\u001b[39m0.1\u001b[39;49m,\u001b[39m1000\u001b[39;49m)\n",
      "\u001b[1;32mc:\\notebooks\\personal\\Machine learning\\Tiny network\\mininetwork.ipynb Celda 21\u001b[0m in \u001b[0;36mgradient_descent\u001b[1;34m(X, Y, alpha, iterations)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Z1,A1,Z2,A2 \u001b[39m=\u001b[39m forward_prop(W1,b1,W2,b2,X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dW1,db1,dW2,db2 \u001b[39m=\u001b[39m backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m W1, b1, W2, b2 \u001b[39m=\u001b[39m update_params(W1,b1, W2,b2,dW1,db1,dW2,db2,alpha)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m i\u001b[39m==\u001b[39miterations\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     predictions\u001b[39m=\u001b[39mget_predictions(A2)\n",
      "\u001b[1;32mc:\\notebooks\\personal\\Machine learning\\Tiny network\\mininetwork.ipynb Celda 21\u001b[0m in \u001b[0;36mupdate_params\u001b[1;34m(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m b1\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mdb1\u001b[39m*\u001b[39malpha\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m W2\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mdW2\u001b[39m*\u001b[39malpha\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m b1\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mdb2\u001b[39m*\u001b[39malpha\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebooks/personal/Machine%20learning/Tiny%20network/mininetwork.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m W1, b1, W2, b2\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (20,1) (10,1) (20,1) "
     ]
    }
   ],
   "source": [
    "labels, data, _,_ =load_data()\n",
    "data=data.T\n",
    "W1,b1,W2,b2,debug=gradient_descent(data,labels,0.1,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:  0.8928\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y=zip(*debug)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([0, None])\n",
    "ax.set_ylim([0, 1])\n",
    "print(\"Final accuracy: \",y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tLabel\tPrediction\tError\n",
      "8\t5\t\t6\t x\n",
      "33\t4\t\t0\t x\n",
      "42\t4\t\t9\t x\n",
      "62\t9\t\t4\t x\n",
      "73\t9\t\t7\t x\n",
      "77\t2\t\t7\t x\n",
      "97\t7\t\t1\t x\n",
      "111\t7\t\t1\t x\n",
      "124\t7\t\t4\t x\n",
      "139\t4\t\t9\t x\n",
      "149\t2\t\t4\t x\n",
      "164\t6\t\t4\t x\n",
      "167\t5\t\t4\t x\n",
      "185\t9\t\t4\t x\n",
      "187\t5\t\t8\t x\n",
      "195\t3\t\t8\t x\n",
      "211\t5\t\t8\t x\n",
      "217\t6\t\t5\t x\n",
      "233\t8\t\t7\t x\n",
      "235\t9\t\t7\t x\n",
      "241\t9\t\t8\t x\n",
      "247\t4\t\t6\t x\n",
      "259\t6\t\t0\t x\n",
      "261\t5\t\t3\t x\n",
      "266\t8\t\t5\t x\n",
      "290\t8\t\t9\t x\n",
      "299\t8\t\t3\t x\n",
      "300\t4\t\t6\t x\n",
      "301\t7\t\t9\t x\n",
      "304\t4\t\t9\t x\n",
      "313\t3\t\t5\t x\n",
      "319\t5\t\t8\t x\n",
      "320\t9\t\t7\t x\n",
      "321\t2\t\t7\t x\n",
      "333\t5\t\t8\t x\n",
      "340\t5\t\t3\t x\n",
      "341\t6\t\t4\t x\n",
      "343\t3\t\t5\t x\n",
      "349\t3\t\t9\t x\n",
      "352\t5\t\t0\t x\n",
      "362\t2\t\t7\t x\n",
      "367\t5\t\t3\t x\n",
      "381\t3\t\t7\t x\n",
      "403\t8\t\t9\t x\n",
      "444\t2\t\t8\t x\n",
      "445\t6\t\t0\t x\n",
      "448\t9\t\t8\t x\n",
      "449\t3\t\t5\t x\n",
      "457\t6\t\t5\t x\n",
      "469\t5\t\t8\t x\n",
      "478\t5\t\t8\t x\n",
      "479\t9\t\t3\t x\n",
      "491\t5\t\t4\t x\n",
      "502\t5\t\t3\t x\n",
      "507\t3\t\t5\t x\n",
      "511\t4\t\t8\t x\n",
      "515\t3\t\t8\t x\n",
      "516\t2\t\t6\t x\n",
      "531\t3\t\t6\t x\n",
      "532\t4\t\t9\t x\n",
      "536\t2\t\t1\t x\n",
      "550\t7\t\t9\t x\n",
      "551\t7\t\t3\t x\n",
      "552\t0\t\t9\t x\n",
      "562\t9\t\t4\t x\n",
      "565\t4\t\t9\t x\n",
      "578\t3\t\t9\t x\n",
      "582\t8\t\t2\t x\n",
      "591\t8\t\t3\t x\n",
      "619\t1\t\t8\t x\n",
      "629\t2\t\t6\t x\n",
      "659\t2\t\t7\t x\n",
      "661\t0\t\t3\t x\n",
      "671\t9\t\t2\t x\n",
      "684\t7\t\t3\t x\n",
      "691\t8\t\t4\t x\n",
      "707\t4\t\t9\t x\n",
      "717\t0\t\t6\t x\n",
      "720\t5\t\t2\t x\n",
      "726\t7\t\t9\t x\n",
      "740\t4\t\t9\t x\n",
      "757\t4\t\t9\t x\n",
      "787\t8\t\t9\t x\n",
      "791\t5\t\t9\t x\n",
      "804\t0\t\t2\t x\n",
      "827\t4\t\t9\t x\n",
      "839\t8\t\t3\t x\n",
      "844\t8\t\t3\t x\n",
      "857\t5\t\t3\t x\n",
      "864\t8\t\t5\t x\n",
      "866\t5\t\t8\t x\n",
      "877\t8\t\t2\t x\n",
      "881\t4\t\t9\t x\n",
      "898\t7\t\t2\t x\n",
      "926\t2\t\t8\t x\n",
      "930\t7\t\t1\t x\n",
      "944\t3\t\t5\t x\n",
      "947\t8\t\t9\t x\n",
      "950\t7\t\t2\t x\n",
      "952\t6\t\t2\t x\n",
      "959\t4\t\t8\t x\n",
      "965\t6\t\t0\t x\n",
      "975\t2\t\t3\t x\n",
      "982\t3\t\t8\t x\n",
      "998\t8\t\t5\t x\n",
      "1000\t9\t\t7\t x\n",
      "1014\t6\t\t0\t x\n",
      "1015\t4\t\t9\t x\n",
      "1024\t4\t\t8\t x\n",
      "1028\t3\t\t5\t x\n",
      "1032\t5\t\t8\t x\n",
      "1033\t8\t\t3\t x\n",
      "1050\t2\t\t6\t x\n",
      "1052\t8\t\t9\t x\n",
      "1055\t7\t\t9\t x\n",
      "1059\t4\t\t9\t x\n",
      "1062\t3\t\t7\t x\n",
      "1082\t5\t\t3\t x\n",
      "1096\t7\t\t0\t x\n",
      "1107\t9\t\t3\t x\n",
      "1112\t4\t\t6\t x\n",
      "1114\t3\t\t8\t x\n",
      "1128\t3\t\t7\t x\n",
      "1130\t9\t\t4\t x\n",
      "1142\t4\t\t9\t x\n",
      "1143\t7\t\t9\t x\n",
      "1146\t5\t\t8\t x\n",
      "1152\t9\t\t7\t x\n",
      "1173\t7\t\t9\t x\n",
      "1176\t0\t\t3\t x\n",
      "1178\t4\t\t6\t x\n",
      "1181\t6\t\t1\t x\n",
      "1191\t0\t\t7\t x\n",
      "1194\t7\t\t9\t x\n",
      "1200\t8\t\t5\t x\n",
      "1202\t8\t\t5\t x\n",
      "1204\t3\t\t8\t x\n",
      "1206\t7\t\t2\t x\n",
      "1209\t6\t\t8\t x\n",
      "1217\t9\t\t7\t x\n",
      "1224\t2\t\t6\t x\n",
      "1226\t7\t\t2\t x\n",
      "1232\t9\t\t6\t x\n",
      "1233\t5\t\t6\t x\n",
      "1234\t8\t\t5\t x\n",
      "1247\t9\t\t0\t x\n",
      "1248\t8\t\t5\t x\n",
      "1256\t2\t\t7\t x\n",
      "1260\t7\t\t1\t x\n",
      "1263\t4\t\t9\t x\n",
      "1269\t2\t\t6\t x\n",
      "1270\t4\t\t9\t x\n",
      "1283\t7\t\t2\t x\n",
      "1291\t3\t\t5\t x\n",
      "1299\t5\t\t7\t x\n",
      "1315\t3\t\t5\t x\n",
      "1319\t8\t\t3\t x\n",
      "1320\t8\t\t3\t x\n",
      "1325\t8\t\t6\t x\n",
      "1326\t7\t\t2\t x\n",
      "1327\t9\t\t4\t x\n",
      "1328\t7\t\t9\t x\n",
      "1330\t4\t\t9\t x\n",
      "1331\t5\t\t8\t x\n",
      "1337\t2\t\t6\t x\n",
      "1339\t5\t\t3\t x\n",
      "1340\t5\t\t8\t x\n",
      "1345\t2\t\t6\t x\n",
      "1355\t7\t\t9\t x\n",
      "1374\t2\t\t6\t x\n",
      "1378\t5\t\t8\t x\n",
      "1384\t3\t\t7\t x\n",
      "1391\t4\t\t9\t x\n",
      "1393\t5\t\t3\t x\n",
      "1402\t2\t\t7\t x\n",
      "1404\t0\t\t3\t x\n",
      "1413\t4\t\t9\t x\n",
      "1421\t5\t\t9\t x\n",
      "1422\t4\t\t9\t x\n",
      "1423\t9\t\t7\t x\n",
      "1425\t8\t\t9\t x\n",
      "1433\t8\t\t1\t x\n",
      "1440\t4\t\t9\t x\n",
      "1447\t5\t\t6\t x\n",
      "1453\t4\t\t9\t x\n",
      "1454\t0\t\t5\t x\n",
      "1455\t9\t\t4\t x\n",
      "1466\t5\t\t3\t x\n",
      "1470\t8\t\t5\t x\n",
      "1474\t8\t\t1\t x\n",
      "1494\t7\t\t0\t x\n",
      "1500\t7\t\t1\t x\n",
      "1522\t7\t\t9\t x\n",
      "1525\t5\t\t0\t x\n",
      "1527\t1\t\t6\t x\n",
      "1530\t8\t\t7\t x\n",
      "1545\t9\t\t3\t x\n",
      "1553\t9\t\t8\t x\n",
      "1554\t9\t\t7\t x\n",
      "1558\t0\t\t5\t x\n",
      "1559\t9\t\t3\t x\n",
      "1575\t4\t\t9\t x\n",
      "1579\t6\t\t8\t x\n",
      "1581\t7\t\t9\t x\n",
      "1584\t8\t\t9\t x\n",
      "1587\t6\t\t5\t x\n",
      "1609\t2\t\t6\t x\n",
      "1634\t4\t\t9\t x\n",
      "1640\t9\t\t4\t x\n",
      "1641\t5\t\t0\t x\n",
      "1678\t2\t\t7\t x\n",
      "1681\t3\t\t7\t x\n",
      "1684\t5\t\t8\t x\n",
      "1695\t9\t\t7\t x\n",
      "1709\t9\t\t5\t x\n",
      "1717\t8\t\t0\t x\n",
      "1718\t7\t\t9\t x\n",
      "1721\t7\t\t9\t x\n",
      "1722\t2\t\t4\t x\n",
      "1724\t4\t\t6\t x\n",
      "1727\t3\t\t7\t x\n",
      "1737\t5\t\t6\t x\n",
      "1741\t7\t\t1\t x\n",
      "1746\t3\t\t5\t x\n",
      "1748\t0\t\t6\t x\n",
      "1751\t4\t\t3\t x\n",
      "1754\t7\t\t1\t x\n",
      "1759\t8\t\t2\t x\n",
      "1765\t3\t\t5\t x\n",
      "1772\t7\t\t4\t x\n",
      "1774\t8\t\t5\t x\n",
      "1782\t8\t\t9\t x\n",
      "1790\t2\t\t7\t x\n",
      "1800\t6\t\t4\t x\n",
      "1801\t9\t\t7\t x\n",
      "1819\t6\t\t0\t x\n",
      "1846\t5\t\t8\t x\n",
      "1849\t4\t\t9\t x\n",
      "1850\t8\t\t9\t x\n",
      "1857\t6\t\t4\t x\n",
      "1868\t1\t\t4\t x\n",
      "1874\t5\t\t8\t x\n",
      "1878\t8\t\t3\t x\n",
      "1896\t5\t\t8\t x\n",
      "1899\t8\t\t3\t x\n",
      "1901\t9\t\t8\t x\n",
      "1910\t5\t\t8\t x\n",
      "1911\t5\t\t0\t x\n",
      "1917\t5\t\t8\t x\n",
      "1926\t3\t\t5\t x\n",
      "1930\t2\t\t6\t x\n",
      "1931\t5\t\t3\t x\n",
      "1938\t4\t\t6\t x\n",
      "1940\t5\t\t0\t x\n",
      "1948\t5\t\t4\t x\n",
      "1952\t9\t\t8\t x\n",
      "1955\t8\t\t2\t x\n",
      "1969\t6\t\t2\t x\n",
      "1970\t5\t\t3\t x\n",
      "1973\t8\t\t5\t x\n",
      "1981\t6\t\t4\t x\n",
      "1984\t2\t\t0\t x\n",
      "2009\t9\t\t8\t x\n",
      "2016\t7\t\t2\t x\n",
      "2035\t5\t\t3\t x\n",
      "2037\t5\t\t3\t x\n",
      "2040\t5\t\t6\t x\n",
      "2043\t4\t\t8\t x\n",
      "2044\t2\t\t7\t x\n",
      "2053\t4\t\t9\t x\n",
      "2063\t7\t\t1\t x\n",
      "2067\t0\t\t6\t x\n",
      "2068\t9\t\t4\t x\n",
      "2070\t7\t\t9\t x\n",
      "2098\t2\t\t0\t x\n",
      "2099\t8\t\t9\t x\n",
      "2105\t3\t\t5\t x\n",
      "2107\t8\t\t0\t x\n",
      "2109\t3\t\t7\t x\n",
      "2115\t7\t\t0\t x\n",
      "2118\t6\t\t0\t x\n",
      "2125\t5\t\t4\t x\n",
      "2129\t9\t\t8\t x\n",
      "2130\t4\t\t9\t x\n",
      "2134\t5\t\t4\t x\n",
      "2135\t6\t\t1\t x\n",
      "2152\t8\t\t3\t x\n",
      "2168\t8\t\t2\t x\n",
      "2182\t1\t\t3\t x\n",
      "2185\t0\t\t8\t x\n",
      "2186\t2\t\t3\t x\n",
      "2189\t9\t\t1\t x\n",
      "2192\t5\t\t3\t x\n",
      "2208\t8\t\t3\t x\n",
      "2218\t4\t\t9\t x\n",
      "2250\t9\t\t4\t x\n",
      "2263\t9\t\t4\t x\n",
      "2266\t1\t\t6\t x\n",
      "2268\t7\t\t5\t x\n",
      "2269\t2\t\t1\t x\n",
      "2272\t8\t\t5\t x\n",
      "2279\t5\t\t3\t x\n",
      "2293\t9\t\t0\t x\n",
      "2299\t2\t\t8\t x\n",
      "2305\t3\t\t8\t x\n",
      "2325\t7\t\t3\t x\n",
      "2345\t9\t\t7\t x\n",
      "2350\t2\t\t6\t x\n",
      "2361\t9\t\t4\t x\n",
      "2362\t8\t\t1\t x\n",
      "2365\t7\t\t9\t x\n",
      "2369\t5\t\t4\t x\n",
      "2371\t4\t\t9\t x\n",
      "2380\t9\t\t2\t x\n",
      "2381\t8\t\t7\t x\n",
      "2386\t4\t\t9\t x\n",
      "2387\t9\t\t7\t x\n",
      "2393\t8\t\t3\t x\n",
      "2395\t8\t\t3\t x\n",
      "2404\t4\t\t6\t x\n",
      "2406\t9\t\t7\t x\n",
      "2422\t6\t\t7\t x\n",
      "2423\t8\t\t9\t x\n",
      "2425\t8\t\t3\t x\n",
      "2433\t2\t\t1\t x\n",
      "2437\t2\t\t1\t x\n",
      "2447\t4\t\t9\t x\n",
      "2449\t0\t\t5\t x\n",
      "2450\t3\t\t5\t x\n",
      "2460\t5\t\t8\t x\n",
      "2488\t2\t\t6\t x\n",
      "2509\t4\t\t9\t x\n",
      "2514\t4\t\t9\t x\n",
      "2516\t9\t\t2\t x\n",
      "2523\t7\t\t9\t x\n",
      "2528\t9\t\t4\t x\n",
      "2534\t3\t\t5\t x\n",
      "2545\t5\t\t3\t x\n",
      "2548\t9\t\t4\t x\n",
      "2552\t8\t\t9\t x\n",
      "2556\t5\t\t8\t x\n",
      "2559\t5\t\t3\t x\n",
      "2560\t3\t\t2\t x\n",
      "2573\t5\t\t8\t x\n",
      "2574\t5\t\t7\t x\n",
      "2582\t9\t\t7\t x\n",
      "2586\t5\t\t3\t x\n",
      "2598\t8\t\t2\t x\n",
      "2607\t7\t\t1\t x\n",
      "2609\t6\t\t4\t x\n",
      "2611\t5\t\t8\t x\n",
      "2616\t5\t\t8\t x\n",
      "2631\t0\t\t6\t x\n",
      "2635\t2\t\t9\t x\n",
      "2654\t6\t\t1\t x\n",
      "2658\t4\t\t6\t x\n",
      "2659\t4\t\t9\t x\n",
      "2668\t5\t\t8\t x\n",
      "2684\t3\t\t7\t x\n",
      "2695\t7\t\t4\t x\n",
      "2705\t1\t\t8\t x\n",
      "2713\t0\t\t8\t x\n",
      "2735\t9\t\t7\t x\n",
      "2750\t6\t\t5\t x\n",
      "2751\t6\t\t0\t x\n",
      "2756\t3\t\t2\t x\n",
      "2758\t8\t\t5\t x\n",
      "2770\t3\t\t5\t x\n",
      "2771\t4\t\t9\t x\n",
      "2778\t4\t\t6\t x\n",
      "2780\t2\t\t3\t x\n",
      "2793\t8\t\t2\t x\n",
      "2801\t6\t\t2\t x\n",
      "2805\t5\t\t9\t x\n",
      "2810\t5\t\t3\t x\n",
      "2820\t7\t\t9\t x\n",
      "2823\t7\t\t4\t x\n",
      "2829\t5\t\t8\t x\n",
      "2832\t5\t\t3\t x\n",
      "2836\t4\t\t7\t x\n",
      "2839\t5\t\t8\t x\n",
      "2850\t5\t\t0\t x\n",
      "2853\t3\t\t8\t x\n",
      "2863\t9\t\t4\t x\n",
      "2866\t6\t\t4\t x\n",
      "2896\t8\t\t0\t x\n",
      "2905\t8\t\t3\t x\n",
      "2906\t3\t\t5\t x\n",
      "2914\t3\t\t8\t x\n",
      "2919\t5\t\t8\t x\n",
      "2921\t3\t\t2\t x\n",
      "2925\t5\t\t0\t x\n",
      "2926\t9\t\t4\t x\n",
      "2927\t3\t\t2\t x\n",
      "2930\t5\t\t3\t x\n",
      "2939\t9\t\t7\t x\n",
      "2945\t3\t\t7\t x\n",
      "2952\t3\t\t5\t x\n",
      "2953\t3\t\t5\t x\n",
      "2970\t5\t\t7\t x\n",
      "2979\t9\t\t7\t x\n",
      "2987\t5\t\t3\t x\n",
      "2989\t3\t\t5\t x\n",
      "2995\t6\t\t8\t x\n",
      "2998\t4\t\t9\t x\n",
      "3005\t9\t\t1\t x\n",
      "3053\t5\t\t8\t x\n",
      "3060\t9\t\t7\t x\n",
      "3066\t4\t\t9\t x\n",
      "3068\t8\t\t1\t x\n",
      "3069\t4\t\t6\t x\n",
      "3073\t1\t\t3\t x\n",
      "3095\t5\t\t8\t x\n",
      "3100\t5\t\t8\t x\n",
      "3102\t5\t\t7\t x\n",
      "3110\t3\t\t5\t x\n",
      "3114\t4\t\t6\t x\n",
      "3130\t6\t\t0\t x\n",
      "3136\t7\t\t8\t x\n",
      "3139\t8\t\t3\t x\n",
      "3145\t5\t\t4\t x\n",
      "3150\t3\t\t8\t x\n",
      "3160\t9\t\t4\t x\n",
      "3171\t5\t\t8\t x\n",
      "3176\t2\t\t7\t x\n",
      "3186\t8\t\t2\t x\n",
      "3189\t7\t\t4\t x\n",
      "3193\t3\t\t2\t x\n",
      "3206\t8\t\t3\t x\n",
      "3228\t4\t\t9\t x\n",
      "3240\t9\t\t8\t x\n",
      "3254\t6\t\t5\t x\n",
      "3269\t6\t\t0\t x\n",
      "3284\t8\t\t7\t x\n",
      "3289\t8\t\t5\t x\n",
      "3304\t6\t\t8\t x\n",
      "3307\t6\t\t4\t x\n",
      "3316\t7\t\t4\t x\n",
      "3329\t7\t\t3\t x\n",
      "3333\t7\t\t9\t x\n",
      "3335\t5\t\t3\t x\n",
      "3342\t6\t\t5\t x\n",
      "3344\t6\t\t4\t x\n",
      "3347\t0\t\t2\t x\n",
      "3358\t0\t\t5\t x\n",
      "3369\t9\t\t7\t x\n",
      "3385\t9\t\t7\t x\n",
      "3392\t9\t\t4\t x\n",
      "3394\t9\t\t2\t x\n",
      "3405\t4\t\t9\t x\n",
      "3410\t4\t\t9\t x\n",
      "3436\t2\t\t1\t x\n",
      "3437\t4\t\t9\t x\n",
      "3447\t6\t\t4\t x\n",
      "3450\t0\t\t5\t x\n",
      "3468\t5\t\t8\t x\n",
      "3475\t3\t\t7\t x\n",
      "3503\t9\t\t5\t x\n",
      "3506\t5\t\t6\t x\n",
      "3519\t8\t\t5\t x\n",
      "3520\t6\t\t4\t x\n",
      "3530\t4\t\t9\t x\n",
      "3533\t4\t\t9\t x\n",
      "3547\t2\t\t3\t x\n",
      "3549\t3\t\t8\t x\n",
      "3552\t5\t\t0\t x\n",
      "3558\t5\t\t0\t x\n",
      "3559\t8\t\t5\t x\n",
      "3565\t5\t\t8\t x\n",
      "3567\t8\t\t5\t x\n",
      "3573\t7\t\t9\t x\n",
      "3578\t2\t\t1\t x\n",
      "3597\t9\t\t3\t x\n",
      "3598\t1\t\t5\t x\n",
      "3599\t2\t\t3\t x\n",
      "3604\t7\t\t6\t x\n",
      "3627\t8\t\t2\t x\n",
      "3629\t8\t\t3\t x\n",
      "3636\t5\t\t9\t x\n",
      "3645\t5\t\t7\t x\n",
      "3662\t8\t\t5\t x\n",
      "3664\t9\t\t4\t x\n",
      "3687\t9\t\t4\t x\n",
      "3688\t6\t\t8\t x\n",
      "3691\t5\t\t8\t x\n",
      "3702\t5\t\t3\t x\n",
      "3709\t9\t\t7\t x\n",
      "3716\t9\t\t3\t x\n",
      "3718\t4\t\t9\t x\n",
      "3725\t6\t\t4\t x\n",
      "3726\t4\t\t9\t x\n",
      "3727\t8\t\t5\t x\n",
      "3730\t7\t\t9\t x\n",
      "3732\t8\t\t1\t x\n",
      "3752\t4\t\t6\t x\n",
      "3763\t5\t\t9\t x\n",
      "3767\t7\t\t2\t x\n",
      "3769\t3\t\t8\t x\n",
      "3776\t5\t\t8\t x\n",
      "3778\t5\t\t0\t x\n",
      "3780\t4\t\t6\t x\n",
      "3791\t9\t\t7\t x\n",
      "3796\t2\t\t8\t x\n",
      "3801\t6\t\t0\t x\n",
      "3806\t5\t\t9\t x\n",
      "3808\t7\t\t3\t x\n",
      "3810\t5\t\t8\t x\n",
      "3811\t2\t\t9\t x\n",
      "3817\t2\t\t8\t x\n",
      "3818\t0\t\t6\t x\n",
      "3820\t9\t\t0\t x\n",
      "3821\t9\t\t4\t x\n",
      "3832\t2\t\t7\t x\n",
      "3833\t8\t\t2\t x\n",
      "3836\t7\t\t9\t x\n",
      "3838\t7\t\t1\t x\n",
      "3848\t7\t\t2\t x\n",
      "3853\t6\t\t5\t x\n",
      "3855\t5\t\t0\t x\n",
      "3856\t9\t\t5\t x\n",
      "3869\t9\t\t4\t x\n",
      "3876\t2\t\t5\t x\n",
      "3893\t5\t\t6\t x\n",
      "3898\t5\t\t8\t x\n",
      "3902\t5\t\t3\t x\n",
      "3906\t1\t\t3\t x\n",
      "3912\t2\t\t6\t x\n",
      "3924\t9\t\t4\t x\n",
      "3926\t9\t\t3\t x\n",
      "3929\t5\t\t8\t x\n",
      "3941\t4\t\t6\t x\n",
      "3943\t3\t\t5\t x\n",
      "3946\t2\t\t1\t x\n",
      "3962\t3\t\t8\t x\n",
      "3968\t5\t\t3\t x\n",
      "3976\t7\t\t1\t x\n",
      "3984\t9\t\t8\t x\n",
      "3985\t9\t\t4\t x\n",
      "3987\t8\t\t3\t x\n",
      "3994\t5\t\t0\t x\n",
      "4000\t9\t\t4\t x\n",
      "4002\t3\t\t8\t x\n",
      "4007\t7\t\t9\t x\n",
      "4013\t1\t\t3\t x\n",
      "4017\t4\t\t9\t x\n",
      "4044\t3\t\t5\t x\n",
      "4053\t7\t\t9\t x\n",
      "4059\t5\t\t8\t x\n",
      "4063\t6\t\t5\t x\n",
      "4065\t0\t\t8\t x\n",
      "4068\t8\t\t3\t x\n",
      "4072\t5\t\t3\t x\n",
      "4075\t8\t\t0\t x\n",
      "4076\t5\t\t8\t x\n",
      "4078\t9\t\t7\t x\n",
      "4086\t8\t\t9\t x\n",
      "4093\t9\t\t4\t x\n",
      "4111\t9\t\t4\t x\n",
      "4118\t5\t\t8\t x\n",
      "4121\t8\t\t3\t x\n",
      "4131\t5\t\t7\t x\n",
      "4139\t6\t\t4\t x\n",
      "4140\t8\t\t3\t x\n",
      "4152\t5\t\t1\t x\n",
      "4156\t2\t\t8\t x\n",
      "4159\t8\t\t3\t x\n",
      "4163\t9\t\t5\t x\n",
      "4173\t2\t\t8\t x\n",
      "4176\t2\t\t6\t x\n",
      "4187\t2\t\t7\t x\n",
      "4193\t6\t\t4\t x\n",
      "4201\t1\t\t7\t x\n",
      "4205\t2\t\t3\t x\n",
      "4211\t6\t\t5\t x\n",
      "4212\t1\t\t7\t x\n",
      "4224\t9\t\t7\t x\n",
      "4238\t7\t\t9\t x\n",
      "4239\t6\t\t5\t x\n",
      "4248\t2\t\t8\t x\n",
      "4261\t5\t\t8\t x\n",
      "4265\t4\t\t9\t x\n",
      "4272\t9\t\t4\t x\n",
      "4279\t2\t\t6\t x\n",
      "4289\t2\t\t7\t x\n",
      "4294\t9\t\t5\t x\n",
      "4300\t5\t\t8\t x\n",
      "4301\t9\t\t0\t x\n",
      "4302\t5\t\t1\t x\n",
      "4306\t3\t\t7\t x\n",
      "4315\t5\t\t4\t x\n",
      "4317\t3\t\t7\t x\n",
      "4341\t2\t\t3\t x\n",
      "4344\t9\t\t4\t x\n",
      "4355\t5\t\t9\t x\n",
      "4358\t9\t\t7\t x\n",
      "4359\t5\t\t7\t x\n",
      "4369\t9\t\t4\t x\n",
      "4374\t5\t\t6\t x\n",
      "4376\t8\t\t5\t x\n",
      "4391\t7\t\t9\t x\n",
      "4403\t0\t\t2\t x\n",
      "4408\t9\t\t7\t x\n",
      "4425\t9\t\t4\t x\n",
      "4426\t9\t\t4\t x\n",
      "4433\t7\t\t3\t x\n",
      "4435\t3\t\t1\t x\n",
      "4451\t2\t\t8\t x\n",
      "4454\t9\t\t7\t x\n",
      "4455\t8\t\t5\t x\n",
      "4461\t5\t\t7\t x\n",
      "4463\t5\t\t8\t x\n",
      "4477\t0\t\t8\t x\n",
      "4487\t7\t\t2\t x\n",
      "4497\t8\t\t7\t x\n",
      "4498\t7\t\t9\t x\n",
      "4503\t2\t\t1\t x\n",
      "4505\t9\t\t4\t x\n",
      "4511\t9\t\t2\t x\n",
      "4523\t8\t\t3\t x\n",
      "4548\t5\t\t2\t x\n",
      "4551\t7\t\t9\t x\n",
      "4552\t3\t\t5\t x\n",
      "4558\t4\t\t6\t x\n",
      "4566\t7\t\t1\t x\n",
      "4571\t6\t\t2\t x\n",
      "4575\t4\t\t0\t x\n",
      "4601\t8\t\t4\t x\n",
      "4605\t3\t\t8\t x\n",
      "4615\t2\t\t4\t x\n",
      "4633\t9\t\t4\t x\n",
      "4639\t8\t\t9\t x\n",
      "4640\t8\t\t7\t x\n",
      "4656\t2\t\t7\t x\n",
      "4657\t3\t\t8\t x\n",
      "4659\t9\t\t4\t x\n",
      "4662\t9\t\t4\t x\n",
      "4671\t8\t\t3\t x\n",
      "4682\t9\t\t4\t x\n",
      "4699\t6\t\t8\t x\n",
      "4702\t6\t\t8\t x\n",
      "4712\t5\t\t8\t x\n",
      "4721\t4\t\t8\t x\n",
      "4722\t5\t\t7\t x\n",
      "4724\t8\t\t5\t x\n",
      "4731\t8\t\t7\t x\n",
      "4735\t9\t\t4\t x\n",
      "4736\t7\t\t2\t x\n",
      "4739\t0\t\t7\t x\n",
      "4746\t7\t\t9\t x\n",
      "4751\t4\t\t6\t x\n",
      "4761\t9\t\t1\t x\n",
      "4777\t8\t\t1\t x\n",
      "4785\t3\t\t8\t x\n",
      "4794\t7\t\t9\t x\n",
      "4798\t6\t\t5\t x\n",
      "4807\t8\t\t3\t x\n",
      "4808\t3\t\t5\t x\n",
      "4823\t9\t\t4\t x\n",
      "4828\t5\t\t8\t x\n",
      "4829\t8\t\t3\t x\n",
      "4837\t7\t\t2\t x\n",
      "4839\t8\t\t2\t x\n",
      "4852\t8\t\t6\t x\n",
      "4863\t8\t\t9\t x\n",
      "4874\t9\t\t0\t x\n",
      "4876\t2\t\t6\t x\n",
      "4880\t0\t\t5\t x\n",
      "4882\t6\t\t2\t x\n",
      "4886\t7\t\t1\t x\n",
      "4896\t4\t\t9\t x\n",
      "4902\t5\t\t9\t x\n",
      "4910\t9\t\t4\t x\n",
      "4915\t5\t\t8\t x\n",
      "4923\t0\t\t5\t x\n",
      "4950\t2\t\t5\t x\n",
      "4952\t6\t\t5\t x\n",
      "4956\t8\t\t4\t x\n",
      "4966\t7\t\t1\t x\n",
      "4968\t9\t\t7\t x\n",
      "4978\t8\t\t9\t x\n",
      "4987\t4\t\t6\t x\n",
      "4990\t3\t\t8\t x\n",
      "5038\t3\t\t8\t x\n",
      "5046\t3\t\t2\t x\n",
      "5054\t3\t\t5\t x\n",
      "5065\t8\t\t2\t x\n",
      "5067\t3\t\t6\t x\n",
      "5068\t4\t\t7\t x\n",
      "5078\t3\t\t8\t x\n",
      "5086\t2\t\t8\t x\n",
      "5100\t9\t\t4\t x\n",
      "5121\t7\t\t9\t x\n",
      "5138\t8\t\t5\t x\n",
      "5140\t3\t\t8\t x\n",
      "5143\t3\t\t6\t x\n",
      "5146\t3\t\t8\t x\n",
      "5165\t0\t\t5\t x\n",
      "5201\t4\t\t9\t x\n",
      "5237\t3\t\t2\t x\n",
      "5278\t8\t\t9\t x\n",
      "5288\t8\t\t9\t x\n",
      "5298\t8\t\t0\t x\n",
      "5299\t9\t\t7\t x\n",
      "5331\t1\t\t8\t x\n",
      "5382\t3\t\t5\t x\n",
      "5409\t4\t\t6\t x\n",
      "5414\t9\t\t7\t x\n",
      "5420\t5\t\t3\t x\n",
      "5440\t4\t\t9\t x\n",
      "5457\t1\t\t2\t x\n",
      "5469\t6\t\t2\t x\n",
      "5579\t5\t\t8\t x\n",
      "5600\t7\t\t9\t x\n",
      "5601\t8\t\t6\t x\n",
      "5608\t5\t\t6\t x\n",
      "5611\t8\t\t6\t x\n",
      "5613\t0\t\t6\t x\n",
      "5620\t7\t\t9\t x\n",
      "5642\t1\t\t5\t x\n",
      "5644\t8\t\t5\t x\n",
      "5649\t7\t\t9\t x\n",
      "5653\t0\t\t2\t x\n",
      "5654\t7\t\t9\t x\n",
      "5676\t4\t\t7\t x\n",
      "5690\t6\t\t4\t x\n",
      "5695\t0\t\t5\t x\n",
      "5714\t7\t\t9\t x\n",
      "5718\t0\t\t5\t x\n",
      "5726\t5\t\t8\t x\n",
      "5734\t3\t\t9\t x\n",
      "5735\t5\t\t4\t x\n",
      "5746\t1\t\t2\t x\n",
      "5749\t8\t\t6\t x\n",
      "5752\t5\t\t3\t x\n",
      "5754\t9\t\t7\t x\n",
      "5771\t8\t\t2\t x\n",
      "5801\t6\t\t2\t x\n",
      "5821\t5\t\t3\t x\n",
      "5824\t3\t\t9\t x\n",
      "5835\t7\t\t9\t x\n",
      "5842\t4\t\t9\t x\n",
      "5845\t7\t\t9\t x\n",
      "5851\t4\t\t9\t x\n",
      "5852\t5\t\t3\t x\n",
      "5858\t7\t\t9\t x\n",
      "5862\t5\t\t3\t x\n",
      "5887\t7\t\t0\t x\n",
      "5888\t4\t\t0\t x\n",
      "5891\t5\t\t6\t x\n",
      "5910\t5\t\t3\t x\n",
      "5912\t3\t\t0\t x\n",
      "5913\t5\t\t3\t x\n",
      "5914\t7\t\t9\t x\n",
      "5922\t5\t\t3\t x\n",
      "5923\t4\t\t9\t x\n",
      "5926\t4\t\t9\t x\n",
      "5938\t6\t\t4\t x\n",
      "5955\t3\t\t5\t x\n",
      "5972\t5\t\t3\t x\n",
      "5973\t3\t\t8\t x\n",
      "5985\t5\t\t8\t x\n",
      "5992\t7\t\t9\t x\n",
      "5996\t9\t\t7\t x\n",
      "6020\t6\t\t4\t x\n",
      "6023\t3\t\t5\t x\n",
      "6024\t8\t\t3\t x\n",
      "6035\t2\t\t0\t x\n",
      "6042\t5\t\t3\t x\n",
      "6059\t3\t\t8\t x\n",
      "6065\t3\t\t8\t x\n",
      "6071\t9\t\t3\t x\n",
      "6080\t8\t\t2\t x\n",
      "6081\t9\t\t5\t x\n",
      "6091\t9\t\t5\t x\n",
      "6101\t1\t\t8\t x\n",
      "6109\t2\t\t1\t x\n",
      "6157\t9\t\t0\t x\n",
      "6166\t9\t\t3\t x\n",
      "6172\t9\t\t0\t x\n",
      "6174\t3\t\t5\t x\n",
      "6243\t7\t\t9\t x\n",
      "6324\t5\t\t6\t x\n",
      "6359\t8\t\t3\t x\n",
      "6385\t5\t\t2\t x\n",
      "6391\t2\t\t6\t x\n",
      "6402\t6\t\t2\t x\n",
      "6418\t2\t\t4\t x\n",
      "6421\t3\t\t2\t x\n",
      "6425\t6\t\t2\t x\n",
      "6426\t0\t\t6\t x\n",
      "6428\t0\t\t6\t x\n",
      "6432\t3\t\t8\t x\n",
      "6434\t7\t\t9\t x\n",
      "6452\t7\t\t9\t x\n",
      "6458\t7\t\t9\t x\n",
      "6480\t2\t\t6\t x\n",
      "6490\t7\t\t9\t x\n",
      "6494\t3\t\t5\t x\n",
      "6495\t8\t\t5\t x\n",
      "6501\t3\t\t5\t x\n",
      "6505\t9\t\t0\t x\n",
      "6517\t3\t\t0\t x\n",
      "6530\t5\t\t7\t x\n",
      "6532\t0\t\t7\t x\n",
      "6555\t8\t\t9\t x\n",
      "6558\t6\t\t2\t x\n",
      "6564\t3\t\t7\t x\n",
      "6568\t9\t\t4\t x\n",
      "6569\t3\t\t2\t x\n",
      "6571\t9\t\t7\t x\n",
      "6572\t1\t\t8\t x\n",
      "6573\t5\t\t3\t x\n",
      "6574\t2\t\t6\t x\n",
      "6576\t7\t\t1\t x\n",
      "6578\t8\t\t1\t x\n",
      "6592\t9\t\t4\t x\n",
      "6597\t0\t\t7\t x\n",
      "6598\t5\t\t2\t x\n",
      "6603\t8\t\t9\t x\n",
      "6610\t9\t\t4\t x\n",
      "6613\t6\t\t4\t x\n",
      "6617\t8\t\t2\t x\n",
      "6625\t8\t\t9\t x\n",
      "6627\t9\t\t4\t x\n",
      "6641\t8\t\t5\t x\n",
      "6642\t9\t\t4\t x\n",
      "6643\t0\t\t3\t x\n",
      "6645\t2\t\t8\t x\n",
      "6650\t9\t\t4\t x\n",
      "6651\t0\t\t8\t x\n",
      "6657\t8\t\t5\t x\n",
      "6688\t1\t\t2\t x\n",
      "6694\t1\t\t8\t x\n",
      "6706\t5\t\t8\t x\n",
      "6721\t2\t\t4\t x\n",
      "6730\t7\t\t9\t x\n",
      "6740\t9\t\t0\t x\n",
      "6741\t7\t\t9\t x\n",
      "6744\t2\t\t8\t x\n",
      "6746\t5\t\t4\t x\n",
      "6775\t5\t\t1\t x\n",
      "6776\t8\t\t6\t x\n",
      "6784\t9\t\t4\t x\n",
      "6785\t2\t\t4\t x\n",
      "6796\t2\t\t7\t x\n",
      "6847\t6\t\t4\t x\n",
      "6872\t4\t\t6\t x\n",
      "6912\t2\t\t1\t x\n",
      "6914\t9\t\t5\t x\n",
      "6919\t9\t\t7\t x\n",
      "6926\t6\t\t4\t x\n",
      "6990\t4\t\t9\t x\n",
      "6997\t4\t\t9\t x\n",
      "7015\t2\t\t6\t x\n",
      "7035\t8\t\t5\t x\n",
      "7071\t9\t\t7\t x\n",
      "7094\t8\t\t9\t x\n",
      "7121\t8\t\t9\t x\n",
      "7130\t3\t\t7\t x\n",
      "7178\t5\t\t0\t x\n",
      "7195\t5\t\t8\t x\n",
      "7198\t8\t\t2\t x\n",
      "7210\t6\t\t8\t x\n",
      "7212\t2\t\t4\t x\n",
      "7216\t0\t\t6\t x\n",
      "7220\t8\t\t2\t x\n",
      "7268\t7\t\t9\t x\n",
      "7338\t4\t\t9\t x\n",
      "7361\t4\t\t9\t x\n",
      "7391\t4\t\t9\t x\n",
      "7432\t7\t\t1\t x\n",
      "7433\t8\t\t2\t x\n",
      "7434\t4\t\t8\t x\n",
      "7436\t3\t\t2\t x\n",
      "7437\t5\t\t8\t x\n",
      "7446\t0\t\t6\t x\n",
      "7451\t5\t\t4\t x\n",
      "7459\t9\t\t8\t x\n",
      "7487\t9\t\t4\t x\n",
      "7494\t4\t\t8\t x\n",
      "7498\t5\t\t3\t x\n",
      "7511\t5\t\t7\t x\n",
      "7514\t8\t\t5\t x\n",
      "7539\t2\t\t8\t x\n",
      "7542\t5\t\t1\t x\n",
      "7565\t7\t\t9\t x\n",
      "7574\t4\t\t6\t x\n",
      "7579\t9\t\t4\t x\n",
      "7580\t9\t\t4\t x\n",
      "7595\t3\t\t8\t x\n",
      "7603\t8\t\t5\t x\n",
      "7620\t3\t\t5\t x\n",
      "7672\t5\t\t8\t x\n",
      "7673\t5\t\t8\t x\n",
      "7678\t2\t\t6\t x\n",
      "7712\t6\t\t2\t x\n",
      "7718\t8\t\t5\t x\n",
      "7720\t8\t\t5\t x\n",
      "7724\t2\t\t6\t x\n",
      "7736\t9\t\t7\t x\n",
      "7779\t5\t\t2\t x\n",
      "7797\t5\t\t2\t x\n",
      "7800\t3\t\t2\t x\n",
      "7808\t5\t\t8\t x\n",
      "7821\t3\t\t2\t x\n",
      "7822\t1\t\t2\t x\n",
      "7826\t5\t\t8\t x\n",
      "7839\t1\t\t2\t x\n",
      "7842\t5\t\t8\t x\n",
      "7849\t3\t\t9\t x\n",
      "7850\t5\t\t8\t x\n",
      "7858\t3\t\t2\t x\n",
      "7859\t5\t\t6\t x\n",
      "7870\t5\t\t8\t x\n",
      "7886\t2\t\t4\t x\n",
      "7888\t5\t\t4\t x\n",
      "7902\t7\t\t9\t x\n",
      "7905\t3\t\t2\t x\n",
      "7915\t7\t\t9\t x\n",
      "7917\t2\t\t6\t x\n",
      "7918\t5\t\t6\t x\n",
      "7921\t8\t\t1\t x\n",
      "7945\t2\t\t6\t x\n",
      "7946\t3\t\t8\t x\n",
      "8036\t2\t\t6\t x\n",
      "8044\t9\t\t7\t x\n",
      "8050\t3\t\t2\t x\n",
      "8072\t5\t\t3\t x\n",
      "8081\t4\t\t6\t x\n",
      "8094\t2\t\t8\t x\n",
      "8095\t4\t\t6\t x\n",
      "8097\t8\t\t5\t x\n",
      "8160\t5\t\t3\t x\n",
      "8165\t2\t\t6\t x\n",
      "8196\t6\t\t0\t x\n",
      "8272\t3\t\t5\t x\n",
      "8277\t3\t\t5\t x\n",
      "8279\t8\t\t9\t x\n",
      "8288\t9\t\t4\t x\n",
      "8298\t9\t\t4\t x\n",
      "8318\t2\t\t7\t x\n",
      "8325\t0\t\t6\t x\n",
      "8327\t5\t\t8\t x\n",
      "8334\t9\t\t4\t x\n",
      "8339\t8\t\t6\t x\n",
      "8353\t2\t\t3\t x\n",
      "8361\t2\t\t9\t x\n",
      "8375\t7\t\t9\t x\n",
      "8378\t4\t\t9\t x\n",
      "8391\t7\t\t9\t x\n",
      "8406\t4\t\t9\t x\n",
      "8408\t8\t\t4\t x\n",
      "8456\t8\t\t5\t x\n",
      "8476\t8\t\t5\t x\n",
      "8520\t4\t\t9\t x\n",
      "8522\t8\t\t6\t x\n",
      "8530\t8\t\t5\t x\n",
      "8639\t2\t\t3\t x\n",
      "8684\t3\t\t8\t x\n",
      "8823\t5\t\t8\t x\n",
      "8863\t5\t\t6\t x\n",
      "8908\t2\t\t8\t x\n",
      "8912\t8\t\t1\t x\n",
      "9007\t3\t\t8\t x\n",
      "9008\t4\t\t9\t x\n",
      "9009\t7\t\t2\t x\n",
      "9010\t2\t\t8\t x\n",
      "9013\t5\t\t8\t x\n",
      "9015\t7\t\t2\t x\n",
      "9019\t7\t\t2\t x\n",
      "9024\t7\t\t2\t x\n",
      "9026\t9\t\t4\t x\n",
      "9035\t5\t\t8\t x\n",
      "9036\t7\t\t2\t x\n",
      "9043\t2\t\t8\t x\n",
      "9045\t7\t\t2\t x\n",
      "9141\t7\t\t4\t x\n",
      "9158\t0\t\t7\t x\n",
      "9168\t2\t\t1\t x\n",
      "9170\t9\t\t4\t x\n",
      "9172\t7\t\t9\t x\n",
      "9182\t3\t\t9\t x\n",
      "9186\t7\t\t9\t x\n",
      "9197\t8\t\t5\t x\n",
      "9210\t6\t\t2\t x\n",
      "9280\t8\t\t5\t x\n",
      "9416\t9\t\t4\t x\n",
      "9422\t5\t\t3\t x\n",
      "9433\t8\t\t6\t x\n",
      "9446\t2\t\t4\t x\n",
      "9456\t2\t\t1\t x\n",
      "9477\t2\t\t6\t x\n",
      "9482\t5\t\t3\t x\n",
      "9513\t5\t\t8\t x\n",
      "9530\t9\t\t8\t x\n",
      "9534\t7\t\t9\t x\n",
      "9535\t2\t\t8\t x\n",
      "9538\t4\t\t9\t x\n",
      "9541\t4\t\t9\t x\n",
      "9546\t7\t\t9\t x\n",
      "9564\t8\t\t2\t x\n",
      "9587\t9\t\t4\t x\n",
      "9595\t2\t\t1\t x\n",
      "9612\t1\t\t7\t x\n",
      "9624\t3\t\t8\t x\n",
      "9634\t0\t\t8\t x\n",
      "9642\t9\t\t7\t x\n",
      "9643\t1\t\t7\t x\n",
      "9651\t5\t\t1\t x\n",
      "9655\t3\t\t2\t x\n",
      "9662\t3\t\t8\t x\n",
      "9670\t3\t\t8\t x\n",
      "9679\t6\t\t5\t x\n",
      "9680\t3\t\t8\t x\n",
      "9684\t7\t\t0\t x\n",
      "9700\t2\t\t8\t x\n",
      "9712\t8\t\t5\t x\n",
      "9716\t2\t\t0\t x\n",
      "9726\t2\t\t0\t x\n",
      "9729\t5\t\t6\t x\n",
      "9733\t9\t\t8\t x\n",
      "9735\t4\t\t1\t x\n",
      "9742\t3\t\t8\t x\n",
      "9744\t8\t\t1\t x\n",
      "9745\t4\t\t2\t x\n",
      "9749\t5\t\t6\t x\n",
      "9751\t2\t\t0\t x\n",
      "9752\t2\t\t0\t x\n",
      "9768\t2\t\t0\t x\n",
      "9770\t5\t\t0\t x\n",
      "9777\t5\t\t0\t x\n",
      "9779\t2\t\t0\t x\n",
      "9780\t8\t\t3\t x\n",
      "9792\t4\t\t9\t x\n",
      "9808\t9\t\t4\t x\n",
      "9811\t2\t\t8\t x\n",
      "9817\t8\t\t1\t x\n",
      "9826\t0\t\t5\t x\n",
      "9829\t9\t\t4\t x\n",
      "9835\t4\t\t9\t x\n",
      "9839\t2\t\t7\t x\n",
      "9840\t3\t\t2\t x\n",
      "9847\t2\t\t8\t x\n",
      "9853\t5\t\t1\t x\n",
      "9855\t2\t\t8\t x\n",
      "9856\t9\t\t5\t x\n",
      "9858\t6\t\t3\t x\n",
      "9862\t6\t\t8\t x\n",
      "9867\t2\t\t8\t x\n",
      "9879\t0\t\t8\t x\n",
      "9883\t5\t\t6\t x\n",
      "9890\t9\t\t4\t x\n",
      "9892\t8\t\t6\t x\n",
      "9893\t2\t\t8\t x\n",
      "9905\t3\t\t7\t x\n",
      "9910\t8\t\t5\t x\n",
      "9925\t3\t\t8\t x\n",
      "9932\t9\t\t7\t x\n",
      "9940\t6\t\t0\t x\n",
      "9941\t5\t\t6\t x\n",
      "9943\t3\t\t8\t x\n",
      "9944\t3\t\t8\t x\n",
      "9945\t9\t\t4\t x\n",
      "9947\t4\t\t6\t x\n",
      "9953\t6\t\t0\t x\n",
      "9967\t8\t\t5\t x\n",
      "9970\t5\t\t3\t x\n",
      "9973\t9\t\t4\t x\n",
      "9975\t3\t\t2\t x\n",
      "9982\t5\t\t2\t x\n",
      "9992\t9\t\t4\t x\n"
     ]
    }
   ],
   "source": [
    "_,_,_,A2=forward_prop(W1,b1,W2,b2,data)\n",
    "predictions=get_predictions(A2)\n",
    "correct=labels==predictions\n",
    "print(\"ID\\tLabel\\tPrediction\\tError\")\n",
    "for i,(lab,pred,right) in enumerate(zip(*(labels,predictions,correct))):\n",
    "    if not right:\n",
    "        print(f\"{i}\\t{lab}\\t\\t{pred}\\t {'x' if not right else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\tPrediction: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=9982\n",
    "plt.imshow(data[:,n].reshape(nrows,ncols), cmap='gray')\n",
    "print(f\"Label: {labels[n]}\\tPrediction: {predictions[n]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas varias de todo tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[0 2 4 6]\n"
     ]
    }
   ],
   "source": [
    "a=np.arange(0,4)\n",
    "b=np.arange(4)*2\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  8, 18])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  2,  4,  6],\n",
       "       [ 0,  4,  8, 12],\n",
       "       [ 0,  6, 12, 18]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.atleast_2d(a).T.dot(np.atleast_2d(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0,  2,  4,  6],\n",
       "       [ 0,  4,  8, 12],\n",
       "       [ 0,  6, 12, 18]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*np.atleast_2d(b).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 10000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data[:4,0:2]\n",
    "x=np.arange(8).reshape((4,2))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz1=one_hot_y[:,0:2]\n",
    "dz1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dz1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwq=dz1.dot(x.T)\n",
    "dwq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [1., 3., 5., 7.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 2., 4., 6.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1],\n",
       "       [1, 1, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=np.arange(9).reshape((3,3))\n",
    "1*(m<5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('calculoastronomico')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45191bebbaaca2a45e1ef8d4f69ed6ce1b861e31669d6b1375d42c01e883be29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
